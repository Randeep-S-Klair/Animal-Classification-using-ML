{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "002b53f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# GPU check\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23840109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Bear', 'Bird', 'Cat', 'Cow', 'Deer', 'Dog', 'Dolphin', 'Elephant', 'Giraffe', 'Horse', 'Kangaroo', 'Lion', 'Panda', 'Tiger', 'Zebra']\n",
      "Total images: 1944\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"C:\\Users\\klair\\Machine Learning\\CV Projects\\Animal Classifiction\\Animal Classification\\dataset\"  # Replace with your dataset path\n",
    "\n",
    "# Get all file paths and labels\n",
    "filepaths, labels = [], []\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file in os.listdir(class_dir):\n",
    "        filepaths.append(os.path.join(class_dir, file))\n",
    "        labels.append(idx)\n",
    "\n",
    "filepaths = np.array(filepaths)\n",
    "labels = np.array(labels)\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Total images:\", len(filepaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "907bd8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 3\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1/3 =====\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "If class_mode=\"sparse\", y_col=\"class\" column values must be strings.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 25\u001b[0m\n\u001b[0;32m     13\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m     14\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[0;32m     15\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m val_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[0;32m     26\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: train_files, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: train_labels}),\n\u001b[0;32m     27\u001b[0m     x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m     y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(IMG_SIZE, IMG_SIZE),\n\u001b[0;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m     31\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m val_gen \u001b[38;5;241m=\u001b[39m val_datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[0;32m     35\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: val_files, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: val_labels}),\n\u001b[0;32m     36\u001b[0m     x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\klair\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1208\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   1202\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates is deprecated, you can drop duplicates \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby using the pandas.DataFrame.drop_duplicates method.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1205\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m   1206\u001b[0m     )\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameIterator(\n\u001b[0;32m   1209\u001b[0m     dataframe,\n\u001b[0;32m   1210\u001b[0m     directory,\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1212\u001b[0m     x_col\u001b[38;5;241m=\u001b[39mx_col,\n\u001b[0;32m   1213\u001b[0m     y_col\u001b[38;5;241m=\u001b[39my_col,\n\u001b[0;32m   1214\u001b[0m     weight_col\u001b[38;5;241m=\u001b[39mweight_col,\n\u001b[0;32m   1215\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m   1216\u001b[0m     color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[0;32m   1217\u001b[0m     classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m   1218\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[0;32m   1219\u001b[0m     data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1220\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1221\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1222\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1223\u001b[0m     save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1224\u001b[0m     save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1225\u001b[0m     save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1226\u001b[0m     subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1227\u001b[0m     interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   1228\u001b[0m     validate_filenames\u001b[38;5;241m=\u001b[39mvalidate_filenames,\n\u001b[0;32m   1229\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1230\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\klair\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:751\u001b[0m, in \u001b[0;36mDataFrameIterator.__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# check that inputs match the required class_mode\u001b[39;00m\n\u001b[1;32m--> 751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(df, x_col, y_col, weight_col, classes)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    753\u001b[0m     validate_filenames\n\u001b[0;32m    754\u001b[0m ):  \u001b[38;5;66;03m# check which image files are valid and keep them\u001b[39;00m\n\u001b[0;32m    755\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_valid_filepaths(df, x_col)\n",
      "File \u001b[1;32mc:\\Users\\klair\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:819\u001b[0m, in \u001b[0;36mDataFrameIterator._check_params\u001b[1;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(df[y_col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m))):\n\u001b[1;32m--> 819\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    820\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf class_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, y_col=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m column \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    821\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mode, y_col)\n\u001b[0;32m    822\u001b[0m         )\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# check that if binary there are only 2 different classes\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: If class_mode=\"sparse\", y_col=\"class\" column values must be strings."
     ]
    }
   ],
   "source": [
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({\n",
    "        'filename': train_files,\n",
    "        'class': train_labels.astype(str)   # <- convert to string\n",
    "    }),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({\n",
    "        'filename': val_files,\n",
    "        'class': val_labels.astype(str)   # <- convert to string\n",
    "    }),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16fcc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all image filepaths and labels\n",
    "filepaths = []\n",
    "labels = []\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file in os.listdir(class_dir):\n",
    "        filepaths.append(os.path.join(class_dir, file))\n",
    "        labels.append(idx)\n",
    "\n",
    "filepaths = np.array(filepaths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "351ef830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1 / 3 =====\n",
      "Found 1296 validated image filenames.\n",
      "Found 648 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\klair\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 533ms/step - accuracy: 0.1609 - loss: 3.0717 - val_accuracy: 0.6327 - val_loss: 1.3242 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 478ms/step - accuracy: 0.5652 - loss: 1.4862 - val_accuracy: 0.7454 - val_loss: 0.8980 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 475ms/step - accuracy: 0.6712 - loss: 1.0704 - val_accuracy: 0.7901 - val_loss: 0.7455 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 477ms/step - accuracy: 0.7432 - loss: 0.8839 - val_accuracy: 0.8148 - val_loss: 0.6536 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 478ms/step - accuracy: 0.7623 - loss: 0.7729 - val_accuracy: 0.8025 - val_loss: 0.6419 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 476ms/step - accuracy: 0.7945 - loss: 0.6788 - val_accuracy: 0.8380 - val_loss: 0.5811 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 476ms/step - accuracy: 0.8027 - loss: 0.6678 - val_accuracy: 0.8349 - val_loss: 0.5674 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 481ms/step - accuracy: 0.7986 - loss: 0.6220 - val_accuracy: 0.8302 - val_loss: 0.5555 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 488ms/step - accuracy: 0.8268 - loss: 0.5519 - val_accuracy: 0.8333 - val_loss: 0.5355 - learning_rate: 0.0010\n",
      "Epoch 1/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 618ms/step - accuracy: 0.7195 - loss: 0.8511 - val_accuracy: 0.8040 - val_loss: 0.6052 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 576ms/step - accuracy: 0.8533 - loss: 0.4680 - val_accuracy: 0.8179 - val_loss: 0.5873 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 657ms/step - accuracy: 0.8972 - loss: 0.3489 - val_accuracy: 0.8272 - val_loss: 0.5374 - learning_rate: 1.0000e-04\n",
      "Fold 1 Accuracy: 0.8071\n",
      "===== Fold 2 / 3 =====\n",
      "Found 1296 validated image filenames.\n",
      "Found 648 validated image filenames.\n",
      "Epoch 1/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 622ms/step - accuracy: 0.1456 - loss: 3.1814 - val_accuracy: 0.5602 - val_loss: 1.4925 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 571ms/step - accuracy: 0.5254 - loss: 1.5720 - val_accuracy: 0.7191 - val_loss: 1.0014 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 574ms/step - accuracy: 0.6647 - loss: 1.0762 - val_accuracy: 0.7639 - val_loss: 0.8207 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 572ms/step - accuracy: 0.7446 - loss: 0.8171 - val_accuracy: 0.7886 - val_loss: 0.7016 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 561ms/step - accuracy: 0.7416 - loss: 0.8312 - val_accuracy: 0.8071 - val_loss: 0.6629 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 570ms/step - accuracy: 0.7845 - loss: 0.6784 - val_accuracy: 0.8148 - val_loss: 0.6257 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 556ms/step - accuracy: 0.8044 - loss: 0.6450 - val_accuracy: 0.8380 - val_loss: 0.5654 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 560ms/step - accuracy: 0.8219 - loss: 0.5416 - val_accuracy: 0.8256 - val_loss: 0.5611 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 556ms/step - accuracy: 0.8176 - loss: 0.5841 - val_accuracy: 0.8503 - val_loss: 0.5379 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 558ms/step - accuracy: 0.8586 - loss: 0.4612 - val_accuracy: 0.8410 - val_loss: 0.5367 - learning_rate: 0.0010\n",
      "Epoch 1/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 696ms/step - accuracy: 0.7529 - loss: 0.8029 - val_accuracy: 0.7870 - val_loss: 0.6483 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 645ms/step - accuracy: 0.8582 - loss: 0.4363 - val_accuracy: 0.8256 - val_loss: 0.5886 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 651ms/step - accuracy: 0.9041 - loss: 0.2883 - val_accuracy: 0.8179 - val_loss: 0.6027 - learning_rate: 1.0000e-04\n",
      "Fold 2 Accuracy: 0.7901\n",
      "===== Fold 3 / 3 =====\n",
      "Found 1296 validated image filenames.\n",
      "Found 648 validated image filenames.\n",
      "Epoch 1/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 633ms/step - accuracy: 0.1785 - loss: 2.9342 - val_accuracy: 0.6296 - val_loss: 1.3191 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 698ms/step - accuracy: 0.5406 - loss: 1.5446 - val_accuracy: 0.7207 - val_loss: 0.9490 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 885ms/step - accuracy: 0.6757 - loss: 1.0826 - val_accuracy: 0.7793 - val_loss: 0.7730 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.7300 - loss: 0.8634 - val_accuracy: 0.7747 - val_loss: 0.6984 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.8082 - val_accuracy: 0.8210 - val_loss: 0.6385 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.7875 - loss: 0.6755 - val_accuracy: 0.8333 - val_loss: 0.5730 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.8186 - loss: 0.6071 - val_accuracy: 0.8318 - val_loss: 0.5394 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 15s/step - accuracy: 0.8034 - loss: 0.6400 - val_accuracy: 0.8426 - val_loss: 0.5358 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 654ms/step - accuracy: 0.8489 - loss: 0.4842 - val_accuracy: 0.8580 - val_loss: 0.4979 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 674ms/step - accuracy: 0.8562 - loss: 0.4541 - val_accuracy: 0.8441 - val_loss: 0.4897 - learning_rate: 0.0010\n",
      "Epoch 1/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 841ms/step - accuracy: 0.7354 - loss: 0.8299 - val_accuracy: 0.8225 - val_loss: 0.5574 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 664ms/step - accuracy: 0.8564 - loss: 0.4501 - val_accuracy: 0.8302 - val_loss: 0.5279 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 688ms/step - accuracy: 0.8948 - loss: 0.3521 - val_accuracy: 0.8210 - val_loss: 0.5918 - learning_rate: 1.0000e-04\n",
      "Fold 3 Accuracy: 0.8148\n",
      "\n",
      "===== Cross Validation Results =====\n",
      "Fold 1: 0.8071\n",
      "Fold 2: 0.7901\n",
      "Fold 3: 0.8148\n",
      "Mean Accuracy: 0.8040\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(filepaths, labels), 1):\n",
    "    print(f\"===== Fold {fold} / 3 =====\")\n",
    "    \n",
    "    train_files = filepaths[train_idx]\n",
    "    train_labels = labels[train_idx]\n",
    "    val_files = filepaths[val_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    \n",
    "    train_gen = datagen.flow_from_dataframe(\n",
    "        dataframe=pd.DataFrame({'filename': train_files, 'class': train_labels}),\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=(img_size,img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_gen = datagen.flow_from_dataframe(\n",
    "        dataframe=pd.DataFrame({'filename': val_files, 'class': val_labels}),\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        target_size=(img_size,img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "    \n",
    "    # Create model\n",
    "    model, base_model = create_model(img_size, num_classes)\n",
    "    \n",
    "    # Train with frozen base\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs_frozen,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fine-tune top layers\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-50]:  # Freeze first layers\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=Adam(1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history_ft = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs_finetune,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    acc = model.evaluate(val_gen, verbose=0)[1]\n",
    "    print(f\"Fold {fold} Accuracy: {acc:.4f}\")\n",
    "    fold_accuracies.append(acc)\n",
    "\n",
    "# Cross-validation results\n",
    "print(\"\\n===== Cross Validation Results =====\")\n",
    "for i, acc in enumerate(fold_accuracies):\n",
    "    print(f\"Fold {i+1}: {acc:.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63496f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
